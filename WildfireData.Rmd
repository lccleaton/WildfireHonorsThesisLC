---
title: "Wildfire Data"
author: "Laina Cleaton"
date: "2025-10-26"
output: html_document
---

```{r setup_tidyverse, eval = TRUE}
library(tidyverse)
```

```{r read_files, eval = TRUE}
evac_zone_status <- read_csv("evac_zone_status_geo_event_map.csv")
evac_zones <- read_csv("evac_zones_gis_evaczone.csv")
geo_events <- read_csv("geo_events_geoevent.csv")
```

```{r evac_zone_status, eval = FALSE}
view(evac_zone_status)
```

```{r evac_zones, eval = FALSE}
view(evac_zones)
```

```{r geo_events, eval = FALSE}
view(geo_events)
```

In order to conduct a temporal and spatial analysis, we will join the **evac_zone_status**, **geo_events**, and **evac_zones** datasets. The 'changelog' datasets are unnecessary for the analysis.

The first join, **evac_zone_status** and **geo_events** (by geo_event_id), is important because **evac_zone_status** does not include information on the wildfire (name, geographical location, etc.). **evac_zone_status** only tracks the evacuation zone's activity over time, so by joining these two tables, we will be able to see the evacuation status changes associated with a specific wildfire.

The second join, **evac_zone_status** and **evac_zones** (by uid_v2), brings in the spatial component. Each evacuation zone's geometry and display name will be attached to its status updates, which means we'll be able to pinpoint the when and where each evacuation occurred. Evacuation boundaries can be visualized, their areas calculated, and they can be overlayed with demographic data.

Altogether, the "core spatial dataset" brings together the components of the who, when, and where of wildfire evacuations; which fire the evacuations were associated with, when they were created, and where they occurred.

```{r install_sf, eval = FALSE}
install.packages("sf")
```

```{r wildfire_joins, eval = TRUE}

library(sf)

# --- Convert CSVs to sf objects using WKT column ---
# evac_zones
evac_zones <- st_as_sf(evac_zones, wkt = "geom", crs = 4326)

# select evac_zone_status columns
evac_zone_status_sel <- evac_zone_status %>%
  rename(status_date = date_created) %>%
  select(uid_v2, geo_event_id, status_date)

# select geo_events columns
geo_events_sel <- geo_events %>%
  rename(geo_event_id = id) %>%
  select(geo_event_id, name, lat, lng)

# select evac_zones columns
evac_zones_sel <- evac_zones %>%
  rename(geom_evac_zone = geom) %>%
  select(uid_v2, display_name, geom_evac_zone)

#join columns
core_wildfire_dataset <- evac_zone_status_sel %>%
  left_join(geo_events_sel, by = "geo_event_id") %>%
  left_join(evac_zones_sel, by = "uid_v2") %>%
  st_as_sf()

```

We cannot use rows with NA in **display_name** because they lack geometry, and could break spatial joins. Therefore, we can filter these NA's out.

```{r filter_core_wildfire_dataset, eval = TRUE}
core_wildfire_geo <- core_wildfire_dataset %>%
  filter(!is.na(display_name))
```

After joining the selected WatchDuty tables into a single dataset, each record now represents a single evacuation status update. This means that there can be multiple entries per evacuation zone, **uid_v2**, as its status changes over time. Thus we will need to summarize those updates into meaningful evacuation timelines:

```         
1. Evacuation start time: the earliest status_date recorded for that zone. 
2. Evacuation end time: the latest status_date recorded.
3. Evacuation duration: the time difference between those two points.
```

This step outputs one row per evacuation zone per fire. Thus we can describe how long communities were under evacuation and visualize the temporal progression of the evacuations.

```{r evac_timeline, eval = TRUE}
evac_timeline <- core_wildfire_geo %>%
  st_drop_geometry() %>%      
  group_by(uid_v2) %>%
  summarise(
    n_updates = n(),
    min_date = min(status_date),
    max_date = max(status_date),
    diff_hours = as.numeric(difftime(max_date, min_date, units = "hours"))
  ) %>%
  arrange(desc(diff_hours))

```


Now I need to check the validity of my geom polygons, to ensure I can filter the data and visualize the results.


```{r polygon_validity_check, eval = TRUE}
library(sf)

# Convert the CSV with WKT geometry to sf safely
evac_zones_sf <- st_as_sf(evac_zones, wkt = "geom", crs = 4326, quiet = TRUE)

# Rename geometry column for clarity
evac_zones_sf <- evac_zones_sf %>%
  rename(geometry = geom)

# Fix invalid polygons (degenerate edges, etc.)
evac_zones_sf <- st_make_valid(evac_zones_sf)

# Drop empty geometries (now works since the column is named correctly)
evac_zones_sf <- evac_zones_sf %>%
  filter(!st_is_empty(geometry))

# Sanity check
print(st_is_valid(evac_zones_sf) %>% table())

```

I have 37324 valid geometries, and 134 invalid geometries, even after conversion. Now I can filter out those invalid geometries.

```{r geometry_filter, eval = TRUE}
evac_zones_sf <- evac_zones_sf %>%
  filter(st_is_valid(geometry))
```

```{r joins_fixed, eval = TRUE}
evac_zone_status_sel <- evac_zone_status %>%
  rename(status_date = date_created) %>%
  select(uid_v2, geo_event_id, status_date)

geo_events_sel <- geo_events %>%
  rename(geo_event_id = id) %>%
  select(geo_event_id, name, lat, lng)

evac_zones_sel <- evac_zones_sf %>%
  select(uid_v2, display_name, geometry)

core_wildfire_geo <- evac_zone_status_sel %>%
  left_join(geo_events_sel, by = "geo_event_id") %>%
  left_join(evac_zones_sel, by = "uid_v2") %>%
  st_as_sf()
```


Next, I wanted to see the distribution of the proportion of all the WatchDuty wildfires. I wanted to see which duration times made up the majority of these evacuations. 

```{r evac_timeline_filter_summary, eval = TRUE}
evac_timeline <- evac_timeline %>%
  mutate(diff_hours = as.numeric(diff_hours)) %>%
  filter(!is.na(diff_hours), diff_hours > 0)
```


```{r evac_timeline_distribution, eval = TRUE}
library(tidyverse)
ggplot(evac_timeline, aes(x = diff_hours)) +
  geom_histogram(
    aes(y = after_stat(count / sum(count))),
    bins = 30,
    fill = "steelblue",
    color = "white",
    alpha = 0.7) +
  labs(
    title = "Evacuation Duration Distribution",
    subtitle = "Proportion of all Evacuation Zones",
    x = "Evacuation Duration (hours)",
    y = "Proportion of Evacuation Zones"
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()
```

[explanation of summary statistics]
As we can see, most of the wildfires were under 3,000 hours. I wanted to see how this overall distribution compares to just wildfires in California.


```{r california_filter, eval = TRUE}
library(sf)

ca_bbox <- st_sfc(
  st_polygon(list(rbind(
    c(-125, 32),
    c(-125, 42),
    c(-114, 42),
    c(-114, 32),
    c(-125, 32)
  ))),
  crs = st_crs(core_wildfire_geo)
)

cali_wildfire_geo <- core_wildfire_geo[st_intersects(core_wildfire_geo, ca_bbox, sparse = FALSE), ]
```

```{r cali_evac_timeline_distribution, eval = TRUE}
library(scales)

evac_timeline <- evac_timeline %>%
  mutate(diff_hours = as.numeric(diff_hours)) %>%
  filter(!is.na(diff_hours), diff_hours > 0)

cali_evac_timeline <- cali_wildfire_geo %>%
  st_drop_geometry() %>%
  group_by(uid_v2) %>%
  summarise(
    n_updates = n(),
    min_date = min(status_date),
    max_date = max(status_date),
    diff_hours = as.numeric(difftime(max_date, min_date, units = "hours"))
  ) %>%
  filter(!is.na(diff_hours), diff_hours > 0)

cali_evac_timeline %>%
  summarise(
    n_zones = n(),
    mean_duration = mean(diff_hours, na.rm = TRUE),
    median_duration = median(diff_hours, na.rm = TRUE),
    sd_duration = sd(diff_hours, na.rm = TRUE),
    min_duration = min(diff_hours, na.rm = TRUE),
    max_duration = max(diff_hours, na.rm = TRUE),
    p25 = quantile(diff_hours, 0.25, na.rm = TRUE),
    p75 = quantile(diff_hours, 0.75, na.rm = TRUE)
  )

ggplot(cali_evac_timeline, aes(x = diff_hours)) +
  geom_histogram(
    aes(y = after_stat(count / sum(count))),   # convert counts to proportions
    bins = 30,
    fill = "darkorange",
    color = "white",
    alpha = 0.7
  ) +
  labs(
    title = "Evacuation Duration Distribution",
    subtitle = "Proportion of California Evacuation Zones",
    x = "Evacuation Duration (hours)",
    y = "Proportion of Evacuation Zones"
  ) +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal(base_size = 13)
```

[explanation of summary statistics]
This distribution is much different. We can see that the California evacuation zones have a much higher evacuation duration on average than the distribution of all locations. We can also see that there are 222 evacuation zones located in California, which is about 49.55% of the total 448 evacuation zones recorded. 

```{r cali_vs_all_distribution, eval = TRUE}

library(scales)

evac_timeline <- evac_timeline %>%
  mutate(diff_hours = as.numeric(diff_hours)) %>%
  filter(!is.na(diff_hours), diff_hours > 0)

evac_timeline_full <- evac_timeline %>%
  mutate(region = "All Zones") %>%
  bind_rows(
    cali_evac_timeline %>%
      mutate(region = "California")
  )

ggplot(evac_timeline_full, aes(x = diff_hours, fill = region)) +
  geom_histogram(
    aes(y = after_stat(count / sum(count))),
    bins = 30,
    alpha = 0.5,   # transparency so both histograms are visible
    position = "identity",  # overlay instead of stacked
    color = "white"
  ) +
  scale_fill_manual(values = c("All Zones" = "steelblue", "California" = "darkorange")) +
  labs(
    title = "Evacuation Duration Distribution: California vs All Zones",
    x = "Evacuation Duration (hours)",
    y = "Proportion of Evacuation Zones",
    fill = "Region"
  ) +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal(base_size = 13)
```



Next, we can visualize the joined dataset, using leaflet. Although California is my place of interest, I was curious about where other fires were located. 


```{r install_leaflet, eval = FALSE}
install.packages("leaflet")
```


```{r leaflet_joins_visualization, eval = FALSE}
library(leaflet)

leaflet(filter(core_wildfire_geo, name %in% top_fires)) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(
    fillColor = ~factor(name),
    color = "#444",
    weight = 1,
    popup = ~paste("<b>Fire:</b>", name, "<br><b>Zone:</b>", display_name)
  )
```

This shows that the fires from WatchDuty data were located in Minnesota (near Duluth, on the western tip of Lake Superior) and California. 






Now that we have all of our data, we can overlay it with demographic and economic data.
